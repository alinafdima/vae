{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('data_dir', '/tmp/data/', 'Directory for storing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_WH_SIZE = 28\n",
    "INPUT_SIZE = IMAGE_WH_SIZE * IMAGE_WH_SIZE\n",
    "HIDDEN_ENCODER_SIZE = 400\n",
    "HIDDEN_DECODER_SIZE = 400\n",
    "LATENT_SPACE_SIZE = 20\n",
    "\n",
    "ADAGRAD_LR = 0.01 # Try with {0.01, 0.02, 0.1}\n",
    "MINIBATCH_SIZE = 100\n",
    "NUMBER_ITERATIONS = 1000\n",
    "\n",
    "INIT_STD_DEV = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def create_W(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=INIT_STD_DEV))\n",
    "\n",
    "def create_b(shape):\n",
    "    return tf.Variable(tf.zeros(shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Layers\n",
    "\n",
    "# Input\n",
    "x = tf.placeholder(tf.float32, [None, INPUT_SIZE])\n",
    "\n",
    "# Encoder\n",
    "W_x_h_enc = create_W([INPUT_SIZE, HIDDEN_ENCODER_SIZE])\n",
    "b_x_h_enc = create_b([HIDDEN_ENCODER_SIZE])\n",
    "h_enc = tf.sigmoid(tf.add(tf.matmul(x, W_x_h_enc), b_x_h_enc))\n",
    "\n",
    "W_h_mu_enc = create_W([HIDDEN_ENCODER_SIZE, LATENT_SPACE_SIZE])\n",
    "b_h_mu_enc = create_b([LATENT_SPACE_SIZE])\n",
    "mu_enc = tf.sigmoid(tf.add(tf.matmul(h_enc, W_h_mu_enc), b_h_mu_enc))\n",
    "\n",
    "W_h_logsigma2_enc = create_W([HIDDEN_ENCODER_SIZE, LATENT_SPACE_SIZE])\n",
    "b_h_logsigma2_enc = create_b([LATENT_SPACE_SIZE])\n",
    "logsigma2_enc = tf.sigmoid(tf.add(tf.matmul(h_enc, W_h_logsigma2_enc), b_h_logsigma2_enc))\n",
    "\n",
    "# Sampler\n",
    "eps_enc = tf.random_normal(shape=tf.shape(mu_enc))\n",
    "sigma_enc = tf.exp(0.5 * logsigma2_enc)\n",
    "z = tf.mul(sigma_enc, eps_enc)\n",
    "\n",
    "# Decoder\n",
    "W_z_h_dec = create_W([LATENT_SPACE_SIZE, HIDDEN_DECODER_SIZE])\n",
    "b_z_h_dec = create_b([HIDDEN_DECODER_SIZE])\n",
    "h_dec = tf.sigmoid(tf.add(tf.matmul(z, W_z_h_dec), b_z_h_dec))\n",
    "\n",
    "W_h_x_dec = create_W([HIDDEN_DECODER_SIZE, INPUT_SIZE])\n",
    "b_h_x_dec = create_b([INPUT_SIZE])\n",
    "x_dec = tf.add(tf.matmul(h_dec, W_h_x_dec), b_h_x_dec)\n",
    "\n",
    "log_p_x_z = tf.reduce_sum(-tf.nn.sigmoid_cross_entropy_with_logits(x_dec, x), reduction_indices=1)\n",
    "KL_q_z_x_vs_p_z = - 0.5 * tf.reduce_sum(1 + logsigma2_enc - tf.square(mu_enc) - tf.square(sigma_enc) , reduction_indices=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_bound = - KL_q_z_x_vs_p_z + log_p_x_z\n",
    "loss = - tf.reduce_mean(lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_it = tf.train.AdagradOptimizer(learning_rate=ADAGRAD_LR).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summaries\n",
    "loss_summ = tf.scalar_summary(\"loss\", loss)\n",
    "summary = tf.merge_all_summaries()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize parameters.\n",
      "Iteration 0 | Loss: 548.909545898\n",
      "Iteration 50 | Loss: 211.465820312\n",
      "Iteration 100 | Loss: 201.980422974\n",
      "Iteration 150 | Loss: 218.528625488\n",
      "Iteration 200 | Loss: 203.234771729\n",
      "Iteration 250 | Loss: 207.564849854\n",
      "Iteration 300 | Loss: 206.415664673\n",
      "Iteration 350 | Loss: 203.245681763\n",
      "Iteration 400 | Loss: 203.622299194\n",
      "Iteration 450 | Loss: 192.473358154\n",
      "Iteration 500 | Loss: 211.761322021\n",
      "Iteration 550 | Loss: 205.576217651\n",
      "Iteration 600 | Loss: 211.391799927\n",
      "Iteration 650 | Loss: 210.300827026\n",
      "Iteration 700 | Loss: 209.532806396\n",
      "Iteration 750 | Loss: 206.712600708\n",
      "Iteration 800 | Loss: 203.39730835\n",
      "Iteration 850 | Loss: 207.924591064\n",
      "Iteration 900 | Loss: 201.052230835\n",
      "Iteration 950 | Loss: 203.578552246\n"
     ]
    }
   ],
   "source": [
    "# Training (this code should be updated to follow the use of FLAGS from here: http://stackoverflow.com/questions/33759623/tensorflow-how-to-restore-a-previously-saved-model-python )\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    summary_writer = tf.train.SummaryWriter('logs', graph=sess.graph)\n",
    "\n",
    "    if os.path.isfile(\"models/model.ckpt\"):\n",
    "        saver.restore(sess, \"models/model.ckpt\")\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        print(\"Initialize parameters.\")\n",
    "    \n",
    "    for it in xrange(NUMBER_ITERATIONS):\n",
    "        minibatch = mnist.train.next_batch(MINIBATCH_SIZE)\n",
    "        cur_train_it, cur_summary, cur_loss = sess.run([train_it, summary, loss], feed_dict={x: minibatch[0]})\n",
    "        summary_writer.add_summary(cur_summary, it)\n",
    "        \n",
    "        if it % 50 == 0:\n",
    "            save_path = saver.save(sess, \"models/model.ckpt\")\n",
    "            print(\"Iteration {0} | Loss: {1}\".format(it, cur_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
