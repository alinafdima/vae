{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define flags\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('DATA_DIR', '/tmp/data/', 'Directory for storing data')\n",
    "flags.DEFINE_string('MODEL_PATH', 'models/model.ckpt', 'Path to the parameters of the trained model')\n",
    "\n",
    "flags.DEFINE_integer('INPUT_SIZE', 784, 'Size of the input image')\n",
    "flags.DEFINE_integer('HIDDEN_ENCODER_SIZE', 400, 'Size of the hidden layer in the encoder')\n",
    "flags.DEFINE_integer('HIDDEN_DECODER_SIZE', 400, 'Size of the hidden layer in the decoder')\n",
    "flags.DEFINE_integer('LATENT_SPACE_SIZE', 20, 'Size of the latent space ')\n",
    "\n",
    "flags.DEFINE_float('ADAGRAD_LR', 0.01, 'Learning rate Adagrad')   # Try with {0.01, 0.02, 0.1}\n",
    "flags.DEFINE_integer('MINIBATCH_SIZE', 100, 'Size of minibatch')\n",
    "flags.DEFINE_integer('NUMBER_ITERATIONS', 1000, 'Number of iterations for optimization')\n",
    "\n",
    "flags.DEFINE_float('INIT_STD_DEV', 0.01, 'Standard deviation for the truncated normal used for initializing the weights')\n",
    "\n",
    "flags.DEFINE_boolean('TRAIN', True, 'If False, uses saved parameters instead of training')\n",
    "flags.DEFINE_boolean('TEST', True, 'If False, does not do testing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(FLAGS.DATA_DIR, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def create_W(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=FLAGS.INIT_STD_DEV))\n",
    "\n",
    "def create_b(shape):\n",
    "    return tf.Variable(tf.zeros(shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Layers\n",
    "\n",
    "# Input\n",
    "x = tf.placeholder(tf.float32, [None, FLAGS.INPUT_SIZE])\n",
    "\n",
    "# Encoder\n",
    "W_x_h_enc = create_W([FLAGS.INPUT_SIZE, FLAGS.HIDDEN_ENCODER_SIZE])\n",
    "b_x_h_enc = create_b([FLAGS.HIDDEN_ENCODER_SIZE])\n",
    "h_enc = tf.sigmoid(tf.add(tf.matmul(x, W_x_h_enc), b_x_h_enc))\n",
    "\n",
    "W_h_mu_enc = create_W([FLAGS.HIDDEN_ENCODER_SIZE, FLAGS.LATENT_SPACE_SIZE])\n",
    "b_h_mu_enc = create_b([FLAGS.LATENT_SPACE_SIZE])\n",
    "mu_enc = tf.sigmoid(tf.add(tf.matmul(h_enc, W_h_mu_enc), b_h_mu_enc))\n",
    "\n",
    "W_h_logsigma2_enc = create_W([FLAGS.HIDDEN_ENCODER_SIZE, FLAGS.LATENT_SPACE_SIZE])\n",
    "b_h_logsigma2_enc = create_b([FLAGS.LATENT_SPACE_SIZE])\n",
    "logsigma2_enc = tf.sigmoid(tf.add(tf.matmul(h_enc, W_h_logsigma2_enc), b_h_logsigma2_enc))\n",
    "\n",
    "# Sampler\n",
    "eps_enc = tf.random_normal(shape=tf.shape(mu_enc))\n",
    "sigma_enc = tf.exp(0.5 * logsigma2_enc)\n",
    "z = tf.mul(sigma_enc, eps_enc)\n",
    "\n",
    "# Decoder\n",
    "W_z_h_dec = create_W([FLAGS.LATENT_SPACE_SIZE, FLAGS.HIDDEN_DECODER_SIZE])\n",
    "b_z_h_dec = create_b([FLAGS.HIDDEN_DECODER_SIZE])\n",
    "h_dec = tf.sigmoid(tf.add(tf.matmul(z, W_z_h_dec), b_z_h_dec))\n",
    "\n",
    "W_h_x_dec = create_W([FLAGS.HIDDEN_DECODER_SIZE, FLAGS.INPUT_SIZE])\n",
    "b_h_x_dec = create_b([FLAGS.INPUT_SIZE])\n",
    "x_dec = tf.add(tf.matmul(h_dec, W_h_x_dec), b_h_x_dec)\n",
    "\n",
    "log_p_x_z = tf.reduce_sum(-tf.nn.sigmoid_cross_entropy_with_logits(x_dec, x), reduction_indices=1)\n",
    "KL_q_z_x_vs_p_z = - 0.5 * tf.reduce_sum(1 + logsigma2_enc - tf.square(mu_enc) - tf.square(sigma_enc) , reduction_indices=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_bound = - KL_q_z_x_vs_p_z + log_p_x_z\n",
    "loss = - tf.reduce_mean(lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_it = tf.train.AdagradOptimizer(learning_rate=FLAGS.ADAGRAD_LR).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summaries\n",
    "loss_summ = tf.scalar_summary(\"loss\", loss)\n",
    "summary = tf.merge_all_summaries()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase.\n",
      "Old model removed.\n",
      "Initialize parameters.\n",
      "Iteration 50 | Loss: 205.370666504\n",
      "Iteration 100 | Loss: 208.255310059\n",
      "Iteration 150 | Loss: 209.598205566\n",
      "Iteration 200 | Loss: 204.414642334\n",
      "Iteration 250 | Loss: 203.965484619\n",
      "Iteration 300 | Loss: 212.777526855\n",
      "Iteration 350 | Loss: 209.411911011\n",
      "Iteration 400 | Loss: 209.389938354\n",
      "Iteration 450 | Loss: 208.12109375\n",
      "Iteration 500 | Loss: 201.428024292\n",
      "Iteration 550 | Loss: 209.164337158\n",
      "Iteration 600 | Loss: 205.992141724\n",
      "Iteration 650 | Loss: 200.91619873\n",
      "Iteration 700 | Loss: 201.880935669\n",
      "Iteration 750 | Loss: 203.261291504\n",
      "Iteration 800 | Loss: 207.340316772\n",
      "Iteration 850 | Loss: 205.966598511\n",
      "Iteration 900 | Loss: 202.52671814\n",
      "Iteration 950 | Loss: 213.999801636\n",
      "Iteration 1000 | Loss: 206.888046265\n",
      "\n",
      "Testing phase.\n",
      "Model restored.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training, Testing\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    summary_writer = tf.train.SummaryWriter('logs', graph=sess.graph)\n",
    "    \n",
    "    if FLAGS.TRAIN:\n",
    "        print(\"Training phase.\")\n",
    "        if os.path.isfile(FLAGS.MODEL_PATH):\n",
    "            os.remove(FLAGS.MODEL_PATH)\n",
    "            print(\"Old model removed.\")\n",
    "            \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        print(\"Initialize parameters.\")\n",
    "        \n",
    "        for it in xrange(FLAGS.NUMBER_ITERATIONS):\n",
    "            minibatch = mnist.train.next_batch(FLAGS.MINIBATCH_SIZE)\n",
    "            cur_train_it, cur_summary, cur_loss = sess.run([train_it, summary, loss], feed_dict={x: minibatch[0]})\n",
    "            summary_writer.add_summary(cur_summary, it)\n",
    "\n",
    "            if (it + 1) % 50 == 0 or (it + 1) == FLAGS.NUMBER_ITERATIONS:\n",
    "                save_path = saver.save(sess, FLAGS.MODEL_PATH)\n",
    "                print(\"Iteration {0} | Loss: {1}\".format(it + 1, cur_loss))\n",
    "        print(\"\")\n",
    "        \n",
    "    if FLAGS.TEST:\n",
    "        print(\"Testing phase.\")\n",
    "        if not os.path.isfile(FLAGS.MODEL_PATH):\n",
    "            print(\"No model found. Please add training phase.\")\n",
    "        else:    \n",
    "            saver.restore(sess, \"models/model.ckpt\")\n",
    "            print(\"Model restored.\")\n",
    "        print(\"\")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
